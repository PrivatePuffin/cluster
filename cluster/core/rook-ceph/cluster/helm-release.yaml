apiVersion: helm.toolkit.fluxcd.io/v2beta2
kind: HelmRelease
metadata:
    name: rook-ceph-cluster
    namespace: rook-ceph
spec:
    interval: 5m
    releaseName: rook-ceph-cluster
    chart:
        spec:
            # renovate: registryUrl=https://charts.rook.io/release
            chart: rook-ceph-cluster
            version: v1.13.6
            sourceRef:
                kind: HelmRepository
                name: rook-ceph-charts
                namespace: flux-system
    install:
        createNamespace: true
        crds: CreateReplace
        remediation:
            retries: 3
    upgrade:
        crds: CreateReplace
        remediation:
            retries: 3
    values:
      toolbox:
        enabled: true
      monitoring:
        enabled: true
        createPrometheusRules: false
      configOverride: |
        [global]
        bdev_enable_discard = true
        bdev_async_discard = true
        bluefs_buffered_io = false
      cephClusterSpec:
        mgr:
          count: 3
          allowMultiplePerNode: false
        mon:
          count: 3
          allowMultiplePerNode: false
        resources:
          mgr:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              cpu: 2
              memory: "1485M"
          mon:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "3390M"
          osd:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "5944M"
          prepareosd:
            limits:
              cpu: 100m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 51Mi
          mgr-sidecar:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "227M"
          crashcollector:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "64M"
          logcollector:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "1G"
          cleanup:
            requests:
              cpu: "25m"
              memory: "49M"
            limits:
              memory: "1G"

        crashCollector:
          disable: false

        dashboard:
          enabled: true
          urlPrefix: /
          ssl: false

        storage:
          useAllNodes: false
          useAllDevices: true
          # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement
          onlyApplyOSDPlacement: false
          nodes:
            - name: k8s-worker-vm-1
              config:
                deviceClass: nvme
                osdsPerDevice: "1"
            - name: k8s-worker-vm-2
              config:
                deviceClass: nvme
                osdsPerDevice: "1"
            - name: k8s-worker-vm-3
              config:
                deviceClass: nvme
                osdsPerDevice: "1"
            - name: k8s-worker-1
            - name: k8s-worker-2
            - name: k8s-worker-3


      ingress:
        dashboard:
          annotations:
            traefik.ingress.kubernetes.io/router.entrypoints: websecure
            cert-manager.io/cluster-issuer: ks-le-prod
            cert-manager.io/private-key-rotation-policy: Always
            traefik.ingress.kubernetes.io/router.tls: 'true'
          tls:
            - hosts:
                - rook.${BASE_DOMAIN}
              secretName: rook-ceph-dashboard
          host:
            name: "rook.${BASE_DOMAIN}"
            path: "/"

      cephBlockPools:
        - name: ceph-nvme
          spec:
            failureDomain: host
            deviceClass: nvme
            enableRBDStats: true
            replicated:
              size: 3
          storageClass:
            enabled: true
            name: ceph-nvme
            isDefault: true
            reclaimPolicy: Retain
            allowVolumeExpansion: true
            parameters:
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
              csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/fstype: ext4
        - name: ceph-nvme-2
          spec:
            failureDomain: host
            deviceClass: nvme
            enableRBDStats: true
            replicated:
              size: 2
          storageClass:
            enabled: true
            name: ceph-nvme-2
            isDefault: false
            reclaimPolicy: Retain
            allowVolumeExpansion: true
            parameters:
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
              csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
              csi.storage.k8s.io/fstype: ext4
        # - name: ceph-hdd
        #   spec:
        #     failureDomain: host
        #     deviceClass: hdd
        #     enableRBDStats: true
        #     replicated:
        #       size: 3
        #   storageClass:
        #     enabled: true
        #     name: ceph-hdd
        #     isDefault: false
        #     reclaimPolicy: Retain
        #     allowVolumeExpansion: true
        #     parameters:
        #       csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        #       csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        #       csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        #       csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        #       csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        #       csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        #       csi.storage.k8s.io/fstype: ext4
        # - name: ceph-hdd-2
        #   spec:
        #     failureDomain: host
        #     deviceClass: hdd
        #     enableRBDStats: true
        #     replicated:
        #       size: 2
        #   storageClass:
        #     enabled: true
        #     name: ceph-hdd-2
        #     isDefault: false
        #     reclaimPolicy: Retain
        #     allowVolumeExpansion: true
        #     parameters:
        #       csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        #       csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
        #       csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        #       csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
        #       csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        #       csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
        #       csi.storage.k8s.io/fstype: ext4
        #
      cephFileSystems:
        - name: ceph-filesystem-nvme-3
          spec:
            metadataPool:
              deviceClass: nvme
              replicated:
                size: 3
            dataPools:
              - failureDomain: host
                name: data0
                deviceClass: nvme
                replicated:
                  size: 3
            metadataServer:
              activeCount: 1
              activeStandby: true
          storageClass:
            enabled: true
            isDefault: false
            name: ceph-filesystem-nvme-3
            reclaimPolicy: Retain
            allowVolumeExpansion: true
            mountOptions: []
        - name: ceph-filesystem-nvme-2
          spec:
            metadataPool:
              deviceClass: nvme
              replicated:
                size: 3
            dataPools:
              - failureDomain: host
                name: data1
                deviceClass: nvme
                replicated:
                  size: 2
            metadataServer:
              activeCount: 1
              activeStandby: true
          storageClass:
            enabled: true
            isDefault: false
            name: ceph-filesystem-nvme-2
            reclaimPolicy: Retain
            allowVolumeExpansion: true
            mountOptions: []
        # - name: ceph-filesystem-hdd-3
        #   spec:
        #     metadataPool:
        #       deviceClass: nvme
        #       replicated:
        #         size: 3
        #     dataPools:
        #       - failureDomain: host
        #         name: data2
        #         deviceClass: hdd
        #         replicated:
        #           size: 3
        #     metadataServer:
        #       activeCount: 1
        #       activeStandby: true
        #   storageClass:
        #     enabled: true
        #     isDefault: false
        #     name: ceph-filesystem-hdd-3
        #     reclaimPolicy: Retain
        #     allowVolumeExpansion: true
        #     mountOptions: []
        # - name: ceph-filesystem-hdd-2
        #   spec:
        #     metadataPool:
        #       deviceClass: nvme
        #       replicated:
        #         size: 3
        #     dataPools:
        #       - failureDomain: host
        #         name: data3
        #         deviceClass: hdd
        #         replicated:
        #           size: 2
        #     metadataServer:
        #       activeCount: 1
        #       activeStandby: true
        #   storageClass:
        #     enabled: true
        #     isDefault: false
        #     name: ceph-filesystem-hdd-2
        #     reclaimPolicy: Retain
        #     allowVolumeExpansion: true


      cephObjectStores: []
